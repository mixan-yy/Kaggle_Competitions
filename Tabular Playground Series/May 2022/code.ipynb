{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900000, 33)\n",
      "(700000, 32)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>...</th>\n",
       "      <th>f_21</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>900000</td>\n",
       "      <td>0.442517</td>\n",
       "      <td>0.174380</td>\n",
       "      <td>-0.999816</td>\n",
       "      <td>0.762741</td>\n",
       "      <td>0.186778</td>\n",
       "      <td>-1.074775</td>\n",
       "      <td>0.501888</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.006400</td>\n",
       "      <td>-1.193879</td>\n",
       "      <td>-2.435736</td>\n",
       "      <td>-2.427430</td>\n",
       "      <td>-1.966887</td>\n",
       "      <td>5.734205</td>\n",
       "      <td>BAAABADLAC</td>\n",
       "      <td>99.478419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>900001</td>\n",
       "      <td>-0.605598</td>\n",
       "      <td>-0.305715</td>\n",
       "      <td>0.627667</td>\n",
       "      <td>-0.578898</td>\n",
       "      <td>-1.750931</td>\n",
       "      <td>1.355550</td>\n",
       "      <td>-0.190911</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.382405</td>\n",
       "      <td>0.149442</td>\n",
       "      <td>1.883322</td>\n",
       "      <td>-2.848714</td>\n",
       "      <td>-0.725155</td>\n",
       "      <td>3.194219</td>\n",
       "      <td>AFABBAEGCB</td>\n",
       "      <td>-65.993825</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      f_00      f_01      f_02      f_03      f_04      f_05  \\\n",
       "0  900000  0.442517  0.174380 -0.999816  0.762741  0.186778 -1.074775   \n",
       "1  900001 -0.605598 -0.305715  0.627667 -0.578898 -1.750931  1.355550   \n",
       "\n",
       "       f_06  f_07  f_08  ...      f_21      f_22      f_23      f_24  \\\n",
       "0  0.501888     6     6  ... -1.006400 -1.193879 -2.435736 -2.427430   \n",
       "1 -0.190911     1     3  ...  2.382405  0.149442  1.883322 -2.848714   \n",
       "\n",
       "       f_25      f_26        f_27       f_28  f_29  f_30  \n",
       "0 -1.966887  5.734205  BAAABADLAC  99.478419     0     0  \n",
       "1 -0.725155  3.194219  AFABBAEGCB -65.993825     1     0  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>...</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.373246</td>\n",
       "      <td>0.238887</td>\n",
       "      <td>-0.243376</td>\n",
       "      <td>0.567405</td>\n",
       "      <td>-0.647715</td>\n",
       "      <td>0.839326</td>\n",
       "      <td>0.113133</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.540739</td>\n",
       "      <td>0.766952</td>\n",
       "      <td>-2.730628</td>\n",
       "      <td>-0.208177</td>\n",
       "      <td>1.363402</td>\n",
       "      <td>ABABDADBAB</td>\n",
       "      <td>67.609153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.697021</td>\n",
       "      <td>-1.710322</td>\n",
       "      <td>-2.230332</td>\n",
       "      <td>-0.545661</td>\n",
       "      <td>1.113173</td>\n",
       "      <td>-1.552175</td>\n",
       "      <td>0.447825</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.278315</td>\n",
       "      <td>-0.633658</td>\n",
       "      <td>-1.217077</td>\n",
       "      <td>-3.782194</td>\n",
       "      <td>-0.058316</td>\n",
       "      <td>ACACCADCEB</td>\n",
       "      <td>377.096415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
       "0   0 -1.373246  0.238887 -0.243376  0.567405 -0.647715  0.839326  0.113133   \n",
       "1   1  1.697021 -1.710322 -2.230332 -0.545661  1.113173 -1.552175  0.447825   \n",
       "\n",
       "   f_07  f_08  ...      f_22      f_23      f_24      f_25      f_26  \\\n",
       "0     1     5  ... -2.540739  0.766952 -2.730628 -0.208177  1.363402   \n",
       "1     1     3  ...  2.278315 -0.633658 -1.217077 -3.782194 -0.058316   \n",
       "\n",
       "         f_27        f_28  f_29  f_30  target  \n",
       "0  ABABDADBAB   67.609153     0     0       0  \n",
       "1  ACACCADCEB  377.096415     0     0       1  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the train data\n",
    "train_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['unique characters'] = train_df.f_27.apply(lambda s: len(set(s)))\n",
    "test_df['unique characters'] = test_df.f_27.apply(lambda s: len(set(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>...</th>\n",
       "      <th>f_22</th>\n",
       "      <th>f_23</th>\n",
       "      <th>f_24</th>\n",
       "      <th>f_25</th>\n",
       "      <th>f_26</th>\n",
       "      <th>f_27</th>\n",
       "      <th>f_28</th>\n",
       "      <th>f_29</th>\n",
       "      <th>f_30</th>\n",
       "      <th>unique characters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>900000</td>\n",
       "      <td>0.442517</td>\n",
       "      <td>0.174380</td>\n",
       "      <td>-0.999816</td>\n",
       "      <td>0.762741</td>\n",
       "      <td>0.186778</td>\n",
       "      <td>-1.074775</td>\n",
       "      <td>0.501888</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.193879</td>\n",
       "      <td>-2.435736</td>\n",
       "      <td>-2.427430</td>\n",
       "      <td>-1.966887</td>\n",
       "      <td>5.734205</td>\n",
       "      <td>BAAABADLAC</td>\n",
       "      <td>99.478419</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>900001</td>\n",
       "      <td>-0.605598</td>\n",
       "      <td>-0.305715</td>\n",
       "      <td>0.627667</td>\n",
       "      <td>-0.578898</td>\n",
       "      <td>-1.750931</td>\n",
       "      <td>1.355550</td>\n",
       "      <td>-0.190911</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149442</td>\n",
       "      <td>1.883322</td>\n",
       "      <td>-2.848714</td>\n",
       "      <td>-0.725155</td>\n",
       "      <td>3.194219</td>\n",
       "      <td>AFABBAEGCB</td>\n",
       "      <td>-65.993825</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      f_00      f_01      f_02      f_03      f_04      f_05  \\\n",
       "0  900000  0.442517  0.174380 -0.999816  0.762741  0.186778 -1.074775   \n",
       "1  900001 -0.605598 -0.305715  0.627667 -0.578898 -1.750931  1.355550   \n",
       "\n",
       "       f_06  f_07  f_08  ...      f_22      f_23      f_24      f_25  \\\n",
       "0  0.501888     6     6  ... -1.193879 -2.435736 -2.427430 -1.966887   \n",
       "1 -0.190911     1     3  ...  0.149442  1.883322 -2.848714 -0.725155   \n",
       "\n",
       "       f_26        f_27       f_28  f_29  f_30  unique characters  \n",
       "0  5.734205  BAAABADLAC  99.478419     0     0                  5  \n",
       "1  3.194219  AFABBAEGCB -65.993825     1     0                  6  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    train_df[f'f_27_{i}'] = train_df.f_27.str.get(i).apply(ord) - ord('A')\n",
    "    test_df[f'f_27_{i}'] = test_df.f_27.str.get(i).apply(ord) - ord('A')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>...</th>\n",
       "      <th>f_27_0</th>\n",
       "      <th>f_27_1</th>\n",
       "      <th>f_27_2</th>\n",
       "      <th>f_27_3</th>\n",
       "      <th>f_27_4</th>\n",
       "      <th>f_27_5</th>\n",
       "      <th>f_27_6</th>\n",
       "      <th>f_27_7</th>\n",
       "      <th>f_27_8</th>\n",
       "      <th>f_27_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.373246</td>\n",
       "      <td>0.238887</td>\n",
       "      <td>-0.243376</td>\n",
       "      <td>0.567405</td>\n",
       "      <td>-0.647715</td>\n",
       "      <td>0.839326</td>\n",
       "      <td>0.113133</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.697021</td>\n",
       "      <td>-1.710322</td>\n",
       "      <td>-2.230332</td>\n",
       "      <td>-0.545661</td>\n",
       "      <td>1.113173</td>\n",
       "      <td>-1.552175</td>\n",
       "      <td>0.447825</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
       "0   0 -1.373246  0.238887 -0.243376  0.567405 -0.647715  0.839326  0.113133   \n",
       "1   1  1.697021 -1.710322 -2.230332 -0.545661  1.113173 -1.552175  0.447825   \n",
       "\n",
       "   f_07  f_08  ...  f_27_0  f_27_1  f_27_2  f_27_3  f_27_4  f_27_5  f_27_6  \\\n",
       "0     1     5  ...       0       1       0       1       3       0       3   \n",
       "1     1     3  ...       0       2       0       2       2       0       3   \n",
       "\n",
       "   f_27_7  f_27_8  f_27_9  \n",
       "0       1       0       1  \n",
       "1       2       4       1  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# le = preprocessing.LabelEncoder()\n",
    "\n",
    "# f_27 = le.fit_transform(train_df['f_27'])\n",
    "# f_27_test = le.fit_transform(test_df['f_27'])\n",
    "\n",
    "# train_df['f_27'] = f_27\n",
    "# test_df['f_27'] = f_27_test\n",
    "\n",
    "# train_df = scaler.fit_transform(train_df)\n",
    "# test_df = scaler.fit_transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    #df.drop('id', axis=1, inplace=True)\n",
    "    # for col in df.columns:\n",
    "    #     if df[col].dtype != 'object' and col!='id':\n",
    "    #         if df[col].max() > 1 or df[col].min() < 0:\n",
    "    #             df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = process_data(train_df)\n",
    "test_df = process_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f_00</th>\n",
       "      <th>f_01</th>\n",
       "      <th>f_02</th>\n",
       "      <th>f_03</th>\n",
       "      <th>f_04</th>\n",
       "      <th>f_05</th>\n",
       "      <th>f_06</th>\n",
       "      <th>f_07</th>\n",
       "      <th>f_08</th>\n",
       "      <th>...</th>\n",
       "      <th>f_27_0</th>\n",
       "      <th>f_27_1</th>\n",
       "      <th>f_27_2</th>\n",
       "      <th>f_27_3</th>\n",
       "      <th>f_27_4</th>\n",
       "      <th>f_27_5</th>\n",
       "      <th>f_27_6</th>\n",
       "      <th>f_27_7</th>\n",
       "      <th>f_27_8</th>\n",
       "      <th>f_27_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.373246</td>\n",
       "      <td>0.238887</td>\n",
       "      <td>-0.243376</td>\n",
       "      <td>0.567405</td>\n",
       "      <td>-0.647715</td>\n",
       "      <td>0.839326</td>\n",
       "      <td>0.113133</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.697021</td>\n",
       "      <td>-1.710322</td>\n",
       "      <td>-2.230332</td>\n",
       "      <td>-0.545661</td>\n",
       "      <td>1.113173</td>\n",
       "      <td>-1.552175</td>\n",
       "      <td>0.447825</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id      f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
       "0   0 -1.373246  0.238887 -0.243376  0.567405 -0.647715  0.839326  0.113133   \n",
       "1   1  1.697021 -1.710322 -2.230332 -0.545661  1.113173 -1.552175  0.447825   \n",
       "\n",
       "   f_07  f_08  ...  f_27_0  f_27_1  f_27_2  f_27_3  f_27_4  f_27_5  f_27_6  \\\n",
       "0     1     5  ...       0       1       0       1       3       0       3   \n",
       "1     1     3  ...       0       2       0       2       2       0       3   \n",
       "\n",
       "   f_27_7  f_27_8  f_27_9  \n",
       "0       1       0       1  \n",
       "1       2       4       1  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700000, 41)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission = test_df.drop(['f_27', 'id'], axis=1).values\n",
    "df_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(['f_27', 'target', 'id'], axis=1).values\n",
    "y_train = train_df['target'].values\n",
    "X_test = test_df.drop(['f_27', 'id'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_estimators=1000, \n",
    "                    learning_rate=0.1, \n",
    "                    max_depth=10, \n",
    "                    subsample=0.8, \n",
    "                    colsample_bytree=0.8,\n",
    "                    objective='binary:logistic',\n",
    "                    n_jobs=-1)\n",
    "cat = CatBoostClassifier(iterations=1000,\n",
    "                            learning_rate=0.1,\n",
    "                            depth=5,\n",
    "                            l2_leaf_reg=3,\n",
    "                            loss_function='Logloss',\n",
    "                            eval_metric='AUC',\n",
    "                            random_seed=42,\n",
    "                            bagging_temperature=0.1,\n",
    "                            od_type='Iter',\n",
    "                            od_wait=100)\n",
    "lgb = LGBMClassifier(n_estimators=1000,\n",
    "                        learning_rate=0.1,  \n",
    "                        max_depth=5,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8,\n",
    "                        objective='binary:logistic',\n",
    "                        n_jobs=-1)\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            max_depth=5,\n",
    "                            random_state=42)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.8,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=10, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=1000,\n",
       "              n_jobs=-1, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = xgb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(y_val, xgb_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9530444444444445"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy report\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.95     92465\n",
      "         1.0       0.95      0.95      0.95     87535\n",
      "\n",
      "    accuracy                           0.95    180000\n",
      "   macro avg       0.95      0.95      0.95    180000\n",
      "weighted avg       0.95      0.95      0.95    180000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/popo/myProjects/Kaggle_Competitions/Tabular Playground Series/May 2022/code.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/popo/myProjects/Kaggle_Competitions/Tabular%20Playground%20Series/May%202022/code.ipynb#ch0000035?line=0'>1</a>\u001b[0m models \u001b[39m=\u001b[39m [xgb, cat, lgb, rf]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/popo/myProjects/Kaggle_Competitions/Tabular%20Playground%20Series/May%202022/code.ipynb#ch0000035?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/popo/myProjects/Kaggle_Competitions/Tabular%20Playground%20Series/May%202022/code.ipynb#ch0000035?line=2'>3</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/popo/myProjects/Kaggle_Competitions/Tabular%20Playground%20Series/May%202022/code.ipynb#ch0000035?line=3'>4</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(X_val)[:,\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/popo/myProjects/Kaggle_Competitions/Tabular%20Playground%20Series/May%202022/code.ipynb#ch0000035?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy_score(y_val, y_pred \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=529'>530</a>\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=530'>531</a>\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=531'>532</a>\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1400\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1378'>1379</a>\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1379'>1380</a>\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1380'>1381</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1381'>1382</a>\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1382'>1383</a>\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1383'>1384</a>\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1396'>1397</a>\u001b[0m     enable_categorical\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menable_categorical,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1397'>1398</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1399'>1400</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1400'>1401</a>\u001b[0m     params,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1401'>1402</a>\u001b[0m     train_dmatrix,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1402'>1403</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1403'>1404</a>\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1404'>1405</a>\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1405'>1406</a>\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1406'>1407</a>\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1407'>1408</a>\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1408'>1409</a>\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1409'>1410</a>\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1410'>1411</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1411'>1412</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1413'>1414</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py?line=1414'>1415</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=529'>530</a>\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=530'>531</a>\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=531'>532</a>\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/training.py?line=178'>179</a>\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/training.py?line=179'>180</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/training.py?line=180'>181</a>\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/training.py?line=181'>182</a>\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/training.py?line=182'>183</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/xgboost/core.py:1733\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=1729'>1730</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_features(dtrain)\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=1731'>1732</a>\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=1732'>1733</a>\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=1733'>1734</a>\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=1734'>1735</a>\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=1735'>1736</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/popo/anaconda3/lib/python3.9/site-packages/xgboost/core.py?line=1736'>1737</a>\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = [xgb, cat, lgb, rf]\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_proba(X_val)[:,1]\n",
    "    print(f'{model.__class__.__name__} accuracy: {accuracy_score(y_val, y_pred > 0.5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.Tensor(y_train).unsqueeze(1)\n",
    "X_val = torch.Tensor(X_val)\n",
    "y_val = torch.Tensor(y_val).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/22500], Loss: 0.4857\n",
      "Epoch [1/100], Step [200/22500], Loss: 0.4606\n",
      "Epoch [1/100], Step [300/22500], Loss: 0.4280\n",
      "Epoch [1/100], Step [400/22500], Loss: 0.4719\n",
      "Epoch [1/100], Step [500/22500], Loss: 0.3987\n",
      "Epoch [1/100], Step [600/22500], Loss: 0.3503\n",
      "Epoch [1/100], Step [700/22500], Loss: 0.3524\n",
      "Epoch [2/100], Step [100/22500], Loss: 0.2970\n",
      "Epoch [2/100], Step [200/22500], Loss: 0.2849\n",
      "Epoch [2/100], Step [300/22500], Loss: 0.2523\n",
      "Epoch [2/100], Step [400/22500], Loss: 0.2757\n",
      "Epoch [2/100], Step [500/22500], Loss: 0.2361\n",
      "Epoch [2/100], Step [600/22500], Loss: 0.1857\n",
      "Epoch [2/100], Step [700/22500], Loss: 0.2414\n",
      "Epoch [3/100], Step [100/22500], Loss: 0.1874\n",
      "Epoch [3/100], Step [200/22500], Loss: 0.1723\n",
      "Epoch [3/100], Step [300/22500], Loss: 0.1957\n",
      "Epoch [3/100], Step [400/22500], Loss: 0.2107\n",
      "Epoch [3/100], Step [500/22500], Loss: 0.2030\n",
      "Epoch [3/100], Step [600/22500], Loss: 0.1553\n",
      "Epoch [3/100], Step [700/22500], Loss: 0.1819\n",
      "Epoch [4/100], Step [100/22500], Loss: 0.1560\n",
      "Epoch [4/100], Step [200/22500], Loss: 0.1162\n",
      "Epoch [4/100], Step [300/22500], Loss: 0.1243\n",
      "Epoch [4/100], Step [400/22500], Loss: 0.1545\n",
      "Epoch [4/100], Step [500/22500], Loss: 0.1460\n",
      "Epoch [4/100], Step [600/22500], Loss: 0.1364\n",
      "Epoch [4/100], Step [700/22500], Loss: 0.1269\n",
      "Epoch [5/100], Step [100/22500], Loss: 0.1357\n",
      "Epoch [5/100], Step [200/22500], Loss: 0.1332\n",
      "Epoch [5/100], Step [300/22500], Loss: 0.1295\n",
      "Epoch [5/100], Step [400/22500], Loss: 0.1662\n",
      "Epoch [5/100], Step [500/22500], Loss: 0.1238\n",
      "Epoch [5/100], Step [600/22500], Loss: 0.1322\n",
      "Epoch [5/100], Step [700/22500], Loss: 0.1702\n",
      "Epoch [6/100], Step [100/22500], Loss: 0.1238\n",
      "Epoch [6/100], Step [200/22500], Loss: 0.1645\n",
      "Epoch [6/100], Step [300/22500], Loss: 0.1193\n",
      "Epoch [6/100], Step [400/22500], Loss: 0.1167\n",
      "Epoch [6/100], Step [500/22500], Loss: 0.1248\n",
      "Epoch [6/100], Step [600/22500], Loss: 0.0872\n",
      "Epoch [6/100], Step [700/22500], Loss: 0.1389\n",
      "Epoch [7/100], Step [100/22500], Loss: 0.1075\n",
      "Epoch [7/100], Step [200/22500], Loss: 0.1154\n",
      "Epoch [7/100], Step [300/22500], Loss: 0.0995\n",
      "Epoch [7/100], Step [400/22500], Loss: 0.0982\n",
      "Epoch [7/100], Step [500/22500], Loss: 0.1086\n",
      "Epoch [7/100], Step [600/22500], Loss: 0.0994\n",
      "Epoch [7/100], Step [700/22500], Loss: 0.1246\n",
      "Epoch [8/100], Step [100/22500], Loss: 0.1002\n",
      "Epoch [8/100], Step [200/22500], Loss: 0.1256\n",
      "Epoch [8/100], Step [300/22500], Loss: 0.1126\n",
      "Epoch [8/100], Step [400/22500], Loss: 0.1134\n",
      "Epoch [8/100], Step [500/22500], Loss: 0.1240\n",
      "Epoch [8/100], Step [600/22500], Loss: 0.1150\n",
      "Epoch [8/100], Step [700/22500], Loss: 0.0963\n",
      "Epoch [9/100], Step [100/22500], Loss: 0.1043\n",
      "Epoch [9/100], Step [200/22500], Loss: 0.1193\n",
      "Epoch [9/100], Step [300/22500], Loss: 0.1029\n",
      "Epoch [9/100], Step [400/22500], Loss: 0.1075\n",
      "Epoch [9/100], Step [500/22500], Loss: 0.1086\n",
      "Epoch [9/100], Step [600/22500], Loss: 0.1134\n",
      "Epoch [9/100], Step [700/22500], Loss: 0.1221\n",
      "Epoch [10/100], Step [100/22500], Loss: 0.1156\n",
      "Epoch [10/100], Step [200/22500], Loss: 0.1103\n",
      "Epoch [10/100], Step [300/22500], Loss: 0.0971\n",
      "Epoch [10/100], Step [400/22500], Loss: 0.1178\n",
      "Epoch [10/100], Step [500/22500], Loss: 0.1242\n",
      "Epoch [10/100], Step [600/22500], Loss: 0.1016\n",
      "Epoch [10/100], Step [700/22500], Loss: 0.0947\n",
      "Epoch [11/100], Step [100/22500], Loss: 0.1108\n",
      "Epoch [11/100], Step [200/22500], Loss: 0.0922\n",
      "Epoch [11/100], Step [300/22500], Loss: 0.0955\n",
      "Epoch [11/100], Step [400/22500], Loss: 0.0866\n",
      "Epoch [11/100], Step [500/22500], Loss: 0.1032\n",
      "Epoch [11/100], Step [600/22500], Loss: 0.0953\n",
      "Epoch [11/100], Step [700/22500], Loss: 0.0951\n",
      "Epoch [12/100], Step [100/22500], Loss: 0.0702\n",
      "Epoch [12/100], Step [200/22500], Loss: 0.0915\n",
      "Epoch [12/100], Step [300/22500], Loss: 0.0933\n",
      "Epoch [12/100], Step [400/22500], Loss: 0.1169\n",
      "Epoch [12/100], Step [500/22500], Loss: 0.0937\n",
      "Epoch [12/100], Step [600/22500], Loss: 0.0962\n",
      "Epoch [12/100], Step [700/22500], Loss: 0.1060\n",
      "Epoch [13/100], Step [100/22500], Loss: 0.0780\n",
      "Epoch [13/100], Step [200/22500], Loss: 0.1186\n",
      "Epoch [13/100], Step [300/22500], Loss: 0.0900\n",
      "Epoch [13/100], Step [400/22500], Loss: 0.1143\n",
      "Epoch [13/100], Step [500/22500], Loss: 0.0949\n",
      "Epoch [13/100], Step [600/22500], Loss: 0.1166\n",
      "Epoch [13/100], Step [700/22500], Loss: 0.0989\n",
      "Epoch [14/100], Step [100/22500], Loss: 0.0903\n",
      "Epoch [14/100], Step [200/22500], Loss: 0.0837\n",
      "Epoch [14/100], Step [300/22500], Loss: 0.0998\n",
      "Epoch [14/100], Step [400/22500], Loss: 0.0919\n",
      "Epoch [14/100], Step [500/22500], Loss: 0.0781\n",
      "Epoch [14/100], Step [600/22500], Loss: 0.1018\n",
      "Epoch [14/100], Step [700/22500], Loss: 0.0845\n",
      "Epoch [15/100], Step [100/22500], Loss: 0.0912\n",
      "Epoch [15/100], Step [200/22500], Loss: 0.0934\n",
      "Epoch [15/100], Step [300/22500], Loss: 0.1087\n",
      "Epoch [15/100], Step [400/22500], Loss: 0.0853\n",
      "Epoch [15/100], Step [500/22500], Loss: 0.0800\n",
      "Epoch [15/100], Step [600/22500], Loss: 0.1150\n",
      "Epoch [15/100], Step [700/22500], Loss: 0.1063\n",
      "Epoch [16/100], Step [100/22500], Loss: 0.0775\n",
      "Epoch [16/100], Step [200/22500], Loss: 0.0820\n",
      "Epoch [16/100], Step [300/22500], Loss: 0.1090\n",
      "Epoch [16/100], Step [400/22500], Loss: 0.1015\n",
      "Epoch [16/100], Step [500/22500], Loss: 0.0737\n",
      "Epoch [16/100], Step [600/22500], Loss: 0.0804\n",
      "Epoch [16/100], Step [700/22500], Loss: 0.0824\n",
      "Epoch [17/100], Step [100/22500], Loss: 0.0621\n",
      "Epoch [17/100], Step [200/22500], Loss: 0.0798\n",
      "Epoch [17/100], Step [300/22500], Loss: 0.0686\n",
      "Epoch [17/100], Step [400/22500], Loss: 0.0964\n",
      "Epoch [17/100], Step [500/22500], Loss: 0.0472\n",
      "Epoch [17/100], Step [600/22500], Loss: 0.0907\n",
      "Epoch [17/100], Step [700/22500], Loss: 0.0853\n",
      "Epoch [18/100], Step [100/22500], Loss: 0.0639\n",
      "Epoch [18/100], Step [200/22500], Loss: 0.0901\n",
      "Epoch [18/100], Step [300/22500], Loss: 0.0971\n",
      "Epoch [18/100], Step [400/22500], Loss: 0.0900\n",
      "Epoch [18/100], Step [500/22500], Loss: 0.0946\n",
      "Epoch [18/100], Step [600/22500], Loss: 0.0792\n",
      "Epoch [18/100], Step [700/22500], Loss: 0.0880\n",
      "Epoch [19/100], Step [100/22500], Loss: 0.0757\n",
      "Epoch [19/100], Step [200/22500], Loss: 0.0689\n",
      "Epoch [19/100], Step [300/22500], Loss: 0.1008\n",
      "Epoch [19/100], Step [400/22500], Loss: 0.0903\n",
      "Epoch [19/100], Step [500/22500], Loss: 0.0934\n",
      "Epoch [19/100], Step [600/22500], Loss: 0.1028\n",
      "Epoch [19/100], Step [700/22500], Loss: 0.0543\n",
      "Epoch [20/100], Step [100/22500], Loss: 0.0769\n",
      "Epoch [20/100], Step [200/22500], Loss: 0.0661\n",
      "Epoch [20/100], Step [300/22500], Loss: 0.0955\n",
      "Epoch [20/100], Step [400/22500], Loss: 0.0734\n",
      "Epoch [20/100], Step [500/22500], Loss: 0.0975\n",
      "Epoch [20/100], Step [600/22500], Loss: 0.0818\n",
      "Epoch [20/100], Step [700/22500], Loss: 0.0737\n",
      "Epoch [21/100], Step [100/22500], Loss: 0.0924\n",
      "Epoch [21/100], Step [200/22500], Loss: 0.1007\n",
      "Epoch [21/100], Step [300/22500], Loss: 0.0646\n",
      "Epoch [21/100], Step [400/22500], Loss: 0.0787\n",
      "Epoch [21/100], Step [500/22500], Loss: 0.0662\n",
      "Epoch [21/100], Step [600/22500], Loss: 0.0731\n",
      "Epoch [21/100], Step [700/22500], Loss: 0.0744\n",
      "Epoch [22/100], Step [100/22500], Loss: 0.0826\n",
      "Epoch [22/100], Step [200/22500], Loss: 0.0828\n",
      "Epoch [22/100], Step [300/22500], Loss: 0.0577\n",
      "Epoch [22/100], Step [400/22500], Loss: 0.0794\n",
      "Epoch [22/100], Step [500/22500], Loss: 0.0791\n",
      "Epoch [22/100], Step [600/22500], Loss: 0.0803\n",
      "Epoch [22/100], Step [700/22500], Loss: 0.0674\n",
      "Epoch [23/100], Step [100/22500], Loss: 0.0811\n",
      "Epoch [23/100], Step [200/22500], Loss: 0.0679\n",
      "Epoch [23/100], Step [300/22500], Loss: 0.0655\n",
      "Epoch [23/100], Step [400/22500], Loss: 0.0723\n",
      "Epoch [23/100], Step [500/22500], Loss: 0.0647\n",
      "Epoch [23/100], Step [600/22500], Loss: 0.0676\n",
      "Epoch [23/100], Step [700/22500], Loss: 0.0756\n",
      "Epoch [24/100], Step [100/22500], Loss: 0.0652\n",
      "Epoch [24/100], Step [200/22500], Loss: 0.0966\n",
      "Epoch [24/100], Step [300/22500], Loss: 0.0663\n",
      "Epoch [24/100], Step [400/22500], Loss: 0.0838\n",
      "Epoch [24/100], Step [500/22500], Loss: 0.0653\n",
      "Epoch [24/100], Step [600/22500], Loss: 0.0779\n",
      "Epoch [24/100], Step [700/22500], Loss: 0.0643\n",
      "Epoch [25/100], Step [100/22500], Loss: 0.0725\n",
      "Epoch [25/100], Step [200/22500], Loss: 0.0579\n",
      "Epoch [25/100], Step [300/22500], Loss: 0.0609\n",
      "Epoch [25/100], Step [400/22500], Loss: 0.0662\n",
      "Epoch [25/100], Step [500/22500], Loss: 0.0569\n",
      "Epoch [25/100], Step [600/22500], Loss: 0.0652\n",
      "Epoch [25/100], Step [700/22500], Loss: 0.0702\n",
      "Epoch [26/100], Step [100/22500], Loss: 0.0859\n",
      "Epoch [26/100], Step [200/22500], Loss: 0.0846\n",
      "Epoch [26/100], Step [300/22500], Loss: 0.0538\n",
      "Epoch [26/100], Step [400/22500], Loss: 0.0794\n",
      "Epoch [26/100], Step [500/22500], Loss: 0.0701\n",
      "Epoch [26/100], Step [600/22500], Loss: 0.0941\n",
      "Epoch [26/100], Step [700/22500], Loss: 0.0723\n",
      "Epoch [27/100], Step [100/22500], Loss: 0.0648\n",
      "Epoch [27/100], Step [200/22500], Loss: 0.0686\n",
      "Epoch [27/100], Step [300/22500], Loss: 0.0893\n",
      "Epoch [27/100], Step [400/22500], Loss: 0.1003\n",
      "Epoch [27/100], Step [500/22500], Loss: 0.0761\n",
      "Epoch [27/100], Step [600/22500], Loss: 0.0908\n",
      "Epoch [27/100], Step [700/22500], Loss: 0.0638\n",
      "Epoch [28/100], Step [100/22500], Loss: 0.0463\n",
      "Epoch [28/100], Step [200/22500], Loss: 0.0473\n",
      "Epoch [28/100], Step [300/22500], Loss: 0.0674\n",
      "Epoch [28/100], Step [400/22500], Loss: 0.0473\n",
      "Epoch [28/100], Step [500/22500], Loss: 0.1085\n",
      "Epoch [28/100], Step [600/22500], Loss: 0.0713\n",
      "Epoch [28/100], Step [700/22500], Loss: 0.0570\n",
      "Epoch [29/100], Step [100/22500], Loss: 0.0681\n",
      "Epoch [29/100], Step [200/22500], Loss: 0.0631\n",
      "Epoch [29/100], Step [300/22500], Loss: 0.0586\n",
      "Epoch [29/100], Step [400/22500], Loss: 0.0612\n",
      "Epoch [29/100], Step [500/22500], Loss: 0.0756\n",
      "Epoch [29/100], Step [600/22500], Loss: 0.0948\n",
      "Epoch [29/100], Step [700/22500], Loss: 0.0757\n",
      "Epoch [30/100], Step [100/22500], Loss: 0.0768\n",
      "Epoch [30/100], Step [200/22500], Loss: 0.0644\n",
      "Epoch [30/100], Step [300/22500], Loss: 0.0683\n",
      "Epoch [30/100], Step [400/22500], Loss: 0.0620\n",
      "Epoch [30/100], Step [500/22500], Loss: 0.0792\n",
      "Epoch [30/100], Step [600/22500], Loss: 0.0592\n",
      "Epoch [30/100], Step [700/22500], Loss: 0.0414\n",
      "Epoch [31/100], Step [100/22500], Loss: 0.0695\n",
      "Epoch [31/100], Step [200/22500], Loss: 0.0661\n",
      "Epoch [31/100], Step [300/22500], Loss: 0.0560\n",
      "Epoch [31/100], Step [400/22500], Loss: 0.0628\n",
      "Epoch [31/100], Step [500/22500], Loss: 0.0687\n",
      "Epoch [31/100], Step [600/22500], Loss: 0.0483\n",
      "Epoch [31/100], Step [700/22500], Loss: 0.0686\n",
      "Epoch [32/100], Step [100/22500], Loss: 0.0637\n",
      "Epoch [32/100], Step [200/22500], Loss: 0.0675\n",
      "Epoch [32/100], Step [300/22500], Loss: 0.0882\n",
      "Epoch [32/100], Step [400/22500], Loss: 0.0758\n",
      "Epoch [32/100], Step [500/22500], Loss: 0.0443\n",
      "Epoch [32/100], Step [600/22500], Loss: 0.0577\n",
      "Epoch [32/100], Step [700/22500], Loss: 0.0663\n",
      "Epoch [33/100], Step [100/22500], Loss: 0.0652\n",
      "Epoch [33/100], Step [200/22500], Loss: 0.0640\n",
      "Epoch [33/100], Step [300/22500], Loss: 0.0575\n",
      "Epoch [33/100], Step [400/22500], Loss: 0.0484\n",
      "Epoch [33/100], Step [500/22500], Loss: 0.0578\n",
      "Epoch [33/100], Step [600/22500], Loss: 0.0476\n",
      "Epoch [33/100], Step [700/22500], Loss: 0.0540\n",
      "Epoch [34/100], Step [100/22500], Loss: 0.0529\n",
      "Epoch [34/100], Step [200/22500], Loss: 0.0689\n",
      "Epoch [34/100], Step [300/22500], Loss: 0.0532\n",
      "Epoch [34/100], Step [400/22500], Loss: 0.0651\n",
      "Epoch [34/100], Step [500/22500], Loss: 0.0428\n",
      "Epoch [34/100], Step [600/22500], Loss: 0.0550\n",
      "Epoch [34/100], Step [700/22500], Loss: 0.0606\n",
      "Epoch [35/100], Step [100/22500], Loss: 0.0647\n",
      "Epoch [35/100], Step [200/22500], Loss: 0.0728\n",
      "Epoch [35/100], Step [300/22500], Loss: 0.0422\n",
      "Epoch [35/100], Step [400/22500], Loss: 0.0426\n",
      "Epoch [35/100], Step [500/22500], Loss: 0.0724\n",
      "Epoch [35/100], Step [600/22500], Loss: 0.0637\n",
      "Epoch [35/100], Step [700/22500], Loss: 0.0604\n",
      "Epoch [36/100], Step [100/22500], Loss: 0.0484\n",
      "Epoch [36/100], Step [200/22500], Loss: 0.0484\n",
      "Epoch [36/100], Step [300/22500], Loss: 0.0591\n",
      "Epoch [36/100], Step [400/22500], Loss: 0.0392\n",
      "Epoch [36/100], Step [500/22500], Loss: 0.0518\n",
      "Epoch [36/100], Step [600/22500], Loss: 0.0504\n",
      "Epoch [36/100], Step [700/22500], Loss: 0.0461\n",
      "Epoch [37/100], Step [100/22500], Loss: 0.0446\n",
      "Epoch [37/100], Step [200/22500], Loss: 0.0461\n",
      "Epoch [37/100], Step [300/22500], Loss: 0.0579\n",
      "Epoch [37/100], Step [400/22500], Loss: 0.0569\n",
      "Epoch [37/100], Step [500/22500], Loss: 0.0582\n",
      "Epoch [37/100], Step [600/22500], Loss: 0.0718\n",
      "Epoch [37/100], Step [700/22500], Loss: 0.0491\n",
      "Epoch [38/100], Step [100/22500], Loss: 0.0642\n",
      "Epoch [38/100], Step [200/22500], Loss: 0.0629\n",
      "Epoch [38/100], Step [300/22500], Loss: 0.0746\n",
      "Epoch [38/100], Step [400/22500], Loss: 0.0680\n",
      "Epoch [38/100], Step [500/22500], Loss: 0.0556\n",
      "Epoch [38/100], Step [600/22500], Loss: 0.0671\n",
      "Epoch [38/100], Step [700/22500], Loss: 0.0549\n",
      "Epoch [39/100], Step [100/22500], Loss: 0.0595\n",
      "Epoch [39/100], Step [200/22500], Loss: 0.0541\n",
      "Epoch [39/100], Step [300/22500], Loss: 0.0827\n",
      "Epoch [39/100], Step [400/22500], Loss: 0.0546\n",
      "Epoch [39/100], Step [500/22500], Loss: 0.0464\n",
      "Epoch [39/100], Step [600/22500], Loss: 0.0741\n",
      "Epoch [39/100], Step [700/22500], Loss: 0.0554\n",
      "Epoch [40/100], Step [100/22500], Loss: 0.0458\n",
      "Epoch [40/100], Step [200/22500], Loss: 0.0409\n",
      "Epoch [40/100], Step [300/22500], Loss: 0.0439\n",
      "Epoch [40/100], Step [400/22500], Loss: 0.0566\n",
      "Epoch [40/100], Step [500/22500], Loss: 0.0551\n",
      "Epoch [40/100], Step [600/22500], Loss: 0.0747\n",
      "Epoch [40/100], Step [700/22500], Loss: 0.0500\n",
      "Epoch [41/100], Step [100/22500], Loss: 0.0469\n",
      "Epoch [41/100], Step [200/22500], Loss: 0.0387\n",
      "Epoch [41/100], Step [300/22500], Loss: 0.0537\n",
      "Epoch [41/100], Step [400/22500], Loss: 0.0682\n",
      "Epoch [41/100], Step [500/22500], Loss: 0.0490\n",
      "Epoch [41/100], Step [600/22500], Loss: 0.0643\n",
      "Epoch [41/100], Step [700/22500], Loss: 0.0621\n",
      "Epoch [42/100], Step [100/22500], Loss: 0.0274\n",
      "Epoch [42/100], Step [200/22500], Loss: 0.0515\n",
      "Epoch [42/100], Step [300/22500], Loss: 0.0482\n",
      "Epoch [42/100], Step [400/22500], Loss: 0.0470\n",
      "Epoch [42/100], Step [500/22500], Loss: 0.0450\n",
      "Epoch [42/100], Step [600/22500], Loss: 0.0453\n",
      "Epoch [42/100], Step [700/22500], Loss: 0.0510\n",
      "Epoch [43/100], Step [100/22500], Loss: 0.0438\n",
      "Epoch [43/100], Step [200/22500], Loss: 0.0518\n",
      "Epoch [43/100], Step [300/22500], Loss: 0.0654\n",
      "Epoch [43/100], Step [400/22500], Loss: 0.0595\n",
      "Epoch [43/100], Step [500/22500], Loss: 0.0543\n",
      "Epoch [43/100], Step [600/22500], Loss: 0.0631\n",
      "Epoch [43/100], Step [700/22500], Loss: 0.0446\n",
      "Epoch [44/100], Step [100/22500], Loss: 0.0391\n",
      "Epoch [44/100], Step [200/22500], Loss: 0.0672\n",
      "Epoch [44/100], Step [300/22500], Loss: 0.0401\n",
      "Epoch [44/100], Step [400/22500], Loss: 0.0709\n",
      "Epoch [44/100], Step [500/22500], Loss: 0.0608\n",
      "Epoch [44/100], Step [600/22500], Loss: 0.0438\n",
      "Epoch [44/100], Step [700/22500], Loss: 0.0465\n",
      "Epoch [45/100], Step [100/22500], Loss: 0.0266\n",
      "Epoch [45/100], Step [200/22500], Loss: 0.0421\n",
      "Epoch [45/100], Step [300/22500], Loss: 0.0412\n",
      "Epoch [45/100], Step [400/22500], Loss: 0.0372\n",
      "Epoch [45/100], Step [500/22500], Loss: 0.0515\n",
      "Epoch [45/100], Step [600/22500], Loss: 0.0719\n",
      "Epoch [45/100], Step [700/22500], Loss: 0.0477\n",
      "Epoch [46/100], Step [100/22500], Loss: 0.0661\n",
      "Epoch [46/100], Step [200/22500], Loss: 0.0550\n",
      "Epoch [46/100], Step [300/22500], Loss: 0.0473\n",
      "Epoch [46/100], Step [400/22500], Loss: 0.0421\n",
      "Epoch [46/100], Step [500/22500], Loss: 0.0547\n",
      "Epoch [46/100], Step [600/22500], Loss: 0.0455\n",
      "Epoch [46/100], Step [700/22500], Loss: 0.0368\n",
      "Epoch [47/100], Step [100/22500], Loss: 0.0427\n",
      "Epoch [47/100], Step [200/22500], Loss: 0.0534\n",
      "Epoch [47/100], Step [300/22500], Loss: 0.0341\n",
      "Epoch [47/100], Step [400/22500], Loss: 0.0457\n",
      "Epoch [47/100], Step [500/22500], Loss: 0.0494\n",
      "Epoch [47/100], Step [600/22500], Loss: 0.0589\n",
      "Epoch [47/100], Step [700/22500], Loss: 0.0549\n",
      "Epoch [48/100], Step [100/22500], Loss: 0.0423\n",
      "Epoch [48/100], Step [200/22500], Loss: 0.0614\n",
      "Epoch [48/100], Step [300/22500], Loss: 0.0468\n",
      "Epoch [48/100], Step [400/22500], Loss: 0.0433\n",
      "Epoch [48/100], Step [500/22500], Loss: 0.0485\n",
      "Epoch [48/100], Step [600/22500], Loss: 0.0362\n",
      "Epoch [48/100], Step [700/22500], Loss: 0.0362\n",
      "Epoch [49/100], Step [100/22500], Loss: 0.0385\n",
      "Epoch [49/100], Step [200/22500], Loss: 0.0390\n",
      "Epoch [49/100], Step [300/22500], Loss: 0.0468\n",
      "Epoch [49/100], Step [400/22500], Loss: 0.0317\n",
      "Epoch [49/100], Step [500/22500], Loss: 0.0652\n",
      "Epoch [49/100], Step [600/22500], Loss: 0.0517\n",
      "Epoch [49/100], Step [700/22500], Loss: 0.0413\n",
      "Epoch [50/100], Step [100/22500], Loss: 0.0334\n",
      "Epoch [50/100], Step [200/22500], Loss: 0.0362\n",
      "Epoch [50/100], Step [300/22500], Loss: 0.0368\n",
      "Epoch [50/100], Step [400/22500], Loss: 0.0358\n",
      "Epoch [50/100], Step [500/22500], Loss: 0.0486\n",
      "Epoch [50/100], Step [600/22500], Loss: 0.0396\n",
      "Epoch [50/100], Step [700/22500], Loss: 0.0830\n",
      "Epoch [51/100], Step [100/22500], Loss: 0.0516\n",
      "Epoch [51/100], Step [200/22500], Loss: 0.0427\n",
      "Epoch [51/100], Step [300/22500], Loss: 0.0467\n",
      "Epoch [51/100], Step [400/22500], Loss: 0.0534\n",
      "Epoch [51/100], Step [500/22500], Loss: 0.0463\n",
      "Epoch [51/100], Step [600/22500], Loss: 0.0536\n",
      "Epoch [51/100], Step [700/22500], Loss: 0.0386\n",
      "Epoch [52/100], Step [100/22500], Loss: 0.0391\n",
      "Epoch [52/100], Step [200/22500], Loss: 0.0381\n",
      "Epoch [52/100], Step [300/22500], Loss: 0.0506\n",
      "Epoch [52/100], Step [400/22500], Loss: 0.0510\n",
      "Epoch [52/100], Step [500/22500], Loss: 0.0380\n",
      "Epoch [52/100], Step [600/22500], Loss: 0.0312\n",
      "Epoch [52/100], Step [700/22500], Loss: 0.0327\n",
      "Epoch [53/100], Step [100/22500], Loss: 0.0460\n",
      "Epoch [53/100], Step [200/22500], Loss: 0.0306\n",
      "Epoch [53/100], Step [300/22500], Loss: 0.0306\n",
      "Epoch [53/100], Step [400/22500], Loss: 0.0342\n",
      "Epoch [53/100], Step [500/22500], Loss: 0.0421\n",
      "Epoch [53/100], Step [600/22500], Loss: 0.0312\n",
      "Epoch [53/100], Step [700/22500], Loss: 0.0390\n",
      "Epoch [54/100], Step [100/22500], Loss: 0.0427\n",
      "Epoch [54/100], Step [200/22500], Loss: 0.0360\n",
      "Epoch [54/100], Step [300/22500], Loss: 0.0290\n",
      "Epoch [54/100], Step [400/22500], Loss: 0.0428\n",
      "Epoch [54/100], Step [500/22500], Loss: 0.0361\n",
      "Epoch [54/100], Step [600/22500], Loss: 0.0503\n",
      "Epoch [54/100], Step [700/22500], Loss: 0.0527\n",
      "Epoch [55/100], Step [100/22500], Loss: 0.0575\n",
      "Epoch [55/100], Step [200/22500], Loss: 0.0458\n",
      "Epoch [55/100], Step [300/22500], Loss: 0.0831\n",
      "Epoch [55/100], Step [400/22500], Loss: 0.0397\n",
      "Epoch [55/100], Step [500/22500], Loss: 0.0670\n",
      "Epoch [55/100], Step [600/22500], Loss: 0.0548\n",
      "Epoch [55/100], Step [700/22500], Loss: 0.0609\n",
      "Epoch [56/100], Step [100/22500], Loss: 0.0312\n",
      "Epoch [56/100], Step [200/22500], Loss: 0.0366\n",
      "Epoch [56/100], Step [300/22500], Loss: 0.0436\n",
      "Epoch [56/100], Step [400/22500], Loss: 0.0490\n",
      "Epoch [56/100], Step [500/22500], Loss: 0.0313\n",
      "Epoch [56/100], Step [600/22500], Loss: 0.0349\n",
      "Epoch [56/100], Step [700/22500], Loss: 0.0314\n",
      "Epoch [57/100], Step [100/22500], Loss: 0.0327\n",
      "Epoch [57/100], Step [200/22500], Loss: 0.0265\n",
      "Epoch [57/100], Step [300/22500], Loss: 0.0445\n",
      "Epoch [57/100], Step [400/22500], Loss: 0.0429\n",
      "Epoch [57/100], Step [500/22500], Loss: 0.0344\n",
      "Epoch [57/100], Step [600/22500], Loss: 0.0326\n",
      "Epoch [57/100], Step [700/22500], Loss: 0.0368\n",
      "Epoch [58/100], Step [100/22500], Loss: 0.0267\n",
      "Epoch [58/100], Step [200/22500], Loss: 0.0902\n",
      "Epoch [58/100], Step [300/22500], Loss: 0.0539\n",
      "Epoch [58/100], Step [400/22500], Loss: 0.0357\n",
      "Epoch [58/100], Step [500/22500], Loss: 0.0326\n",
      "Epoch [58/100], Step [600/22500], Loss: 0.0319\n",
      "Epoch [58/100], Step [700/22500], Loss: 0.0258\n",
      "Epoch [59/100], Step [100/22500], Loss: 0.0539\n",
      "Epoch [59/100], Step [200/22500], Loss: 0.0394\n",
      "Epoch [59/100], Step [300/22500], Loss: 0.0605\n",
      "Epoch [59/100], Step [400/22500], Loss: 0.0328\n",
      "Epoch [59/100], Step [500/22500], Loss: 0.0415\n",
      "Epoch [59/100], Step [600/22500], Loss: 0.0372\n",
      "Epoch [59/100], Step [700/22500], Loss: 0.0339\n",
      "Epoch [60/100], Step [100/22500], Loss: 0.0366\n",
      "Epoch [60/100], Step [200/22500], Loss: 0.0446\n",
      "Epoch [60/100], Step [300/22500], Loss: 0.0379\n",
      "Epoch [60/100], Step [400/22500], Loss: 0.0258\n",
      "Epoch [60/100], Step [500/22500], Loss: 0.0371\n",
      "Epoch [60/100], Step [600/22500], Loss: 0.0502\n",
      "Epoch [60/100], Step [700/22500], Loss: 0.0486\n",
      "Epoch [61/100], Step [100/22500], Loss: 0.0533\n",
      "Epoch [61/100], Step [200/22500], Loss: 0.0451\n",
      "Epoch [61/100], Step [300/22500], Loss: 0.0331\n",
      "Epoch [61/100], Step [400/22500], Loss: 0.0350\n",
      "Epoch [61/100], Step [500/22500], Loss: 0.0231\n",
      "Epoch [61/100], Step [600/22500], Loss: 0.0550\n",
      "Epoch [61/100], Step [700/22500], Loss: 0.0472\n",
      "Epoch [62/100], Step [100/22500], Loss: 0.0316\n",
      "Epoch [62/100], Step [200/22500], Loss: 0.0390\n",
      "Epoch [62/100], Step [300/22500], Loss: 0.0326\n",
      "Epoch [62/100], Step [400/22500], Loss: 0.0268\n",
      "Epoch [62/100], Step [500/22500], Loss: 0.0550\n",
      "Epoch [62/100], Step [600/22500], Loss: 0.0362\n",
      "Epoch [62/100], Step [700/22500], Loss: 0.0400\n",
      "Epoch [63/100], Step [100/22500], Loss: 0.0877\n",
      "Epoch [63/100], Step [200/22500], Loss: 0.0380\n",
      "Epoch [63/100], Step [300/22500], Loss: 0.0399\n",
      "Epoch [63/100], Step [400/22500], Loss: 0.0281\n",
      "Epoch [63/100], Step [500/22500], Loss: 0.0312\n",
      "Epoch [63/100], Step [600/22500], Loss: 0.0498\n",
      "Epoch [63/100], Step [700/22500], Loss: 0.0384\n",
      "Epoch [64/100], Step [100/22500], Loss: 0.0309\n",
      "Epoch [64/100], Step [200/22500], Loss: 0.0317\n",
      "Epoch [64/100], Step [300/22500], Loss: 0.0211\n",
      "Epoch [64/100], Step [400/22500], Loss: 0.0244\n",
      "Epoch [64/100], Step [500/22500], Loss: 0.0277\n",
      "Epoch [64/100], Step [600/22500], Loss: 0.0322\n",
      "Epoch [64/100], Step [700/22500], Loss: 0.0302\n",
      "Epoch [65/100], Step [100/22500], Loss: 0.0202\n",
      "Epoch [65/100], Step [200/22500], Loss: 0.0488\n",
      "Epoch [65/100], Step [300/22500], Loss: 0.0380\n",
      "Epoch [65/100], Step [400/22500], Loss: 0.0364\n",
      "Epoch [65/100], Step [500/22500], Loss: 0.0398\n",
      "Epoch [65/100], Step [600/22500], Loss: 0.0446\n",
      "Epoch [65/100], Step [700/22500], Loss: 0.0397\n",
      "Epoch [66/100], Step [100/22500], Loss: 0.0238\n",
      "Epoch [66/100], Step [200/22500], Loss: 0.0392\n",
      "Epoch [66/100], Step [300/22500], Loss: 0.0294\n",
      "Epoch [66/100], Step [400/22500], Loss: 0.0324\n",
      "Epoch [66/100], Step [500/22500], Loss: 0.0397\n",
      "Epoch [66/100], Step [600/22500], Loss: 0.0307\n",
      "Epoch [66/100], Step [700/22500], Loss: 0.0355\n",
      "Epoch [67/100], Step [100/22500], Loss: 0.0446\n",
      "Epoch [67/100], Step [200/22500], Loss: 0.0435\n",
      "Epoch [67/100], Step [300/22500], Loss: 0.0269\n",
      "Epoch [67/100], Step [400/22500], Loss: 0.0332\n",
      "Epoch [67/100], Step [500/22500], Loss: 0.0399\n",
      "Epoch [67/100], Step [600/22500], Loss: 0.0439\n",
      "Epoch [67/100], Step [700/22500], Loss: 0.0397\n",
      "Epoch [68/100], Step [100/22500], Loss: 0.0356\n",
      "Epoch [68/100], Step [200/22500], Loss: 0.0254\n",
      "Epoch [68/100], Step [300/22500], Loss: 0.0548\n",
      "Epoch [68/100], Step [400/22500], Loss: 0.0236\n",
      "Epoch [68/100], Step [500/22500], Loss: 0.0294\n",
      "Epoch [68/100], Step [600/22500], Loss: 0.0212\n",
      "Epoch [68/100], Step [700/22500], Loss: 0.0446\n",
      "Epoch [69/100], Step [100/22500], Loss: 0.0411\n",
      "Epoch [69/100], Step [200/22500], Loss: 0.0225\n",
      "Epoch [69/100], Step [300/22500], Loss: 0.0361\n",
      "Epoch [69/100], Step [400/22500], Loss: 0.0230\n",
      "Epoch [69/100], Step [500/22500], Loss: 0.0220\n",
      "Epoch [69/100], Step [600/22500], Loss: 0.0360\n",
      "Epoch [69/100], Step [700/22500], Loss: 0.0377\n",
      "Epoch [70/100], Step [100/22500], Loss: 0.0308\n",
      "Epoch [70/100], Step [200/22500], Loss: 0.0209\n",
      "Epoch [70/100], Step [300/22500], Loss: 0.0336\n",
      "Epoch [70/100], Step [400/22500], Loss: 0.0355\n",
      "Epoch [70/100], Step [500/22500], Loss: 0.0367\n",
      "Epoch [70/100], Step [600/22500], Loss: 0.0231\n",
      "Epoch [70/100], Step [700/22500], Loss: 0.0343\n",
      "Epoch [71/100], Step [100/22500], Loss: 0.0167\n",
      "Epoch [71/100], Step [200/22500], Loss: 0.0310\n",
      "Epoch [71/100], Step [300/22500], Loss: 0.0334\n",
      "Epoch [71/100], Step [400/22500], Loss: 0.0485\n",
      "Epoch [71/100], Step [500/22500], Loss: 0.0271\n",
      "Epoch [71/100], Step [600/22500], Loss: 0.0309\n",
      "Epoch [71/100], Step [700/22500], Loss: 0.0229\n",
      "Epoch [72/100], Step [100/22500], Loss: 0.0282\n",
      "Epoch [72/100], Step [200/22500], Loss: 0.0327\n",
      "Epoch [72/100], Step [300/22500], Loss: 0.0289\n",
      "Epoch [72/100], Step [400/22500], Loss: 0.0358\n",
      "Epoch [72/100], Step [500/22500], Loss: 0.0183\n",
      "Epoch [72/100], Step [600/22500], Loss: 0.0261\n",
      "Epoch [72/100], Step [700/22500], Loss: 0.0210\n",
      "Epoch [73/100], Step [100/22500], Loss: 0.0374\n",
      "Epoch [73/100], Step [200/22500], Loss: 0.0277\n",
      "Epoch [73/100], Step [300/22500], Loss: 0.0412\n",
      "Epoch [73/100], Step [400/22500], Loss: 0.0373\n",
      "Epoch [73/100], Step [500/22500], Loss: 0.0456\n",
      "Epoch [73/100], Step [600/22500], Loss: 0.0429\n",
      "Epoch [73/100], Step [700/22500], Loss: 0.0294\n",
      "Epoch [74/100], Step [100/22500], Loss: 0.0338\n",
      "Epoch [74/100], Step [200/22500], Loss: 0.0196\n",
      "Epoch [74/100], Step [300/22500], Loss: 0.0395\n",
      "Epoch [74/100], Step [400/22500], Loss: 0.0328\n",
      "Epoch [74/100], Step [500/22500], Loss: 0.0294\n",
      "Epoch [74/100], Step [600/22500], Loss: 0.0325\n",
      "Epoch [74/100], Step [700/22500], Loss: 0.0240\n",
      "Epoch [75/100], Step [100/22500], Loss: 0.0592\n",
      "Epoch [75/100], Step [200/22500], Loss: 0.0323\n",
      "Epoch [75/100], Step [300/22500], Loss: 0.0376\n",
      "Epoch [75/100], Step [400/22500], Loss: 0.0259\n",
      "Epoch [75/100], Step [500/22500], Loss: 0.0255\n",
      "Epoch [75/100], Step [600/22500], Loss: 0.0256\n",
      "Epoch [75/100], Step [700/22500], Loss: 0.0209\n",
      "Epoch [76/100], Step [100/22500], Loss: 0.0174\n",
      "Epoch [76/100], Step [200/22500], Loss: 0.0210\n",
      "Epoch [76/100], Step [300/22500], Loss: 0.0276\n",
      "Epoch [76/100], Step [400/22500], Loss: 0.0303\n",
      "Epoch [76/100], Step [500/22500], Loss: 0.0361\n",
      "Epoch [76/100], Step [600/22500], Loss: 0.0223\n",
      "Epoch [76/100], Step [700/22500], Loss: 0.0265\n",
      "Epoch [77/100], Step [100/22500], Loss: 0.0243\n",
      "Epoch [77/100], Step [200/22500], Loss: 0.0211\n",
      "Epoch [77/100], Step [300/22500], Loss: 0.0286\n",
      "Epoch [77/100], Step [400/22500], Loss: 0.0322\n",
      "Epoch [77/100], Step [500/22500], Loss: 0.0319\n",
      "Epoch [77/100], Step [600/22500], Loss: 0.0215\n",
      "Epoch [77/100], Step [700/22500], Loss: 0.0356\n",
      "Epoch [78/100], Step [100/22500], Loss: 0.0277\n",
      "Epoch [78/100], Step [200/22500], Loss: 0.0301\n",
      "Epoch [78/100], Step [300/22500], Loss: 0.0233\n",
      "Epoch [78/100], Step [400/22500], Loss: 0.0252\n",
      "Epoch [78/100], Step [500/22500], Loss: 0.0250\n",
      "Epoch [78/100], Step [600/22500], Loss: 0.0307\n",
      "Epoch [78/100], Step [700/22500], Loss: 0.0260\n",
      "Epoch [79/100], Step [100/22500], Loss: 0.0235\n",
      "Epoch [79/100], Step [200/22500], Loss: 0.0285\n",
      "Epoch [79/100], Step [300/22500], Loss: 0.0178\n",
      "Epoch [79/100], Step [400/22500], Loss: 0.0264\n",
      "Epoch [79/100], Step [500/22500], Loss: 0.0218\n",
      "Epoch [79/100], Step [600/22500], Loss: 0.0214\n",
      "Epoch [79/100], Step [700/22500], Loss: 0.0279\n",
      "Epoch [80/100], Step [100/22500], Loss: 0.0370\n",
      "Epoch [80/100], Step [200/22500], Loss: 0.0253\n",
      "Epoch [80/100], Step [300/22500], Loss: 0.0198\n",
      "Epoch [80/100], Step [400/22500], Loss: 0.0350\n",
      "Epoch [80/100], Step [500/22500], Loss: 0.0268\n",
      "Epoch [80/100], Step [600/22500], Loss: 0.0546\n",
      "Epoch [80/100], Step [700/22500], Loss: 0.0286\n",
      "Epoch [81/100], Step [100/22500], Loss: 0.0152\n",
      "Epoch [81/100], Step [200/22500], Loss: 0.0208\n",
      "Epoch [81/100], Step [300/22500], Loss: 0.0204\n",
      "Epoch [81/100], Step [400/22500], Loss: 0.0197\n",
      "Epoch [81/100], Step [500/22500], Loss: 0.0224\n",
      "Epoch [81/100], Step [600/22500], Loss: 0.0334\n",
      "Epoch [81/100], Step [700/22500], Loss: 0.0151\n",
      "Epoch [82/100], Step [100/22500], Loss: 0.0472\n",
      "Epoch [82/100], Step [200/22500], Loss: 0.0222\n",
      "Epoch [82/100], Step [300/22500], Loss: 0.0380\n",
      "Epoch [82/100], Step [400/22500], Loss: 0.0306\n",
      "Epoch [82/100], Step [500/22500], Loss: 0.0259\n",
      "Epoch [82/100], Step [600/22500], Loss: 0.0095\n",
      "Epoch [82/100], Step [700/22500], Loss: 0.0263\n",
      "Epoch [83/100], Step [100/22500], Loss: 0.0285\n",
      "Epoch [83/100], Step [200/22500], Loss: 0.0302\n",
      "Epoch [83/100], Step [300/22500], Loss: 0.0221\n",
      "Epoch [83/100], Step [400/22500], Loss: 0.0321\n",
      "Epoch [83/100], Step [500/22500], Loss: 0.0213\n",
      "Epoch [83/100], Step [600/22500], Loss: 0.0289\n",
      "Epoch [83/100], Step [700/22500], Loss: 0.0210\n",
      "Epoch [84/100], Step [100/22500], Loss: 0.0194\n",
      "Epoch [84/100], Step [200/22500], Loss: 0.0276\n",
      "Epoch [84/100], Step [300/22500], Loss: 0.0274\n",
      "Epoch [84/100], Step [400/22500], Loss: 0.0233\n",
      "Epoch [84/100], Step [500/22500], Loss: 0.0214\n",
      "Epoch [84/100], Step [600/22500], Loss: 0.0277\n",
      "Epoch [84/100], Step [700/22500], Loss: 0.0455\n",
      "Epoch [85/100], Step [100/22500], Loss: 0.0287\n",
      "Epoch [85/100], Step [200/22500], Loss: 0.0272\n",
      "Epoch [85/100], Step [300/22500], Loss: 0.0257\n",
      "Epoch [85/100], Step [400/22500], Loss: 0.0202\n",
      "Epoch [85/100], Step [500/22500], Loss: 0.0261\n",
      "Epoch [85/100], Step [600/22500], Loss: 0.0424\n",
      "Epoch [85/100], Step [700/22500], Loss: 0.0422\n",
      "Epoch [86/100], Step [100/22500], Loss: 0.0232\n",
      "Epoch [86/100], Step [200/22500], Loss: 0.0356\n",
      "Epoch [86/100], Step [300/22500], Loss: 0.0156\n",
      "Epoch [86/100], Step [400/22500], Loss: 0.0184\n",
      "Epoch [86/100], Step [500/22500], Loss: 0.0212\n",
      "Epoch [86/100], Step [600/22500], Loss: 0.0241\n",
      "Epoch [86/100], Step [700/22500], Loss: 0.0180\n",
      "Epoch [87/100], Step [100/22500], Loss: 0.0152\n",
      "Epoch [87/100], Step [200/22500], Loss: 0.0302\n",
      "Epoch [87/100], Step [300/22500], Loss: 0.0204\n",
      "Epoch [87/100], Step [400/22500], Loss: 0.0166\n",
      "Epoch [87/100], Step [500/22500], Loss: 0.0290\n",
      "Epoch [87/100], Step [600/22500], Loss: 0.0282\n",
      "Epoch [87/100], Step [700/22500], Loss: 0.0223\n",
      "Epoch [88/100], Step [100/22500], Loss: 0.0178\n",
      "Epoch [88/100], Step [200/22500], Loss: 0.0173\n",
      "Epoch [88/100], Step [300/22500], Loss: 0.0204\n",
      "Epoch [88/100], Step [400/22500], Loss: 0.0300\n",
      "Epoch [88/100], Step [500/22500], Loss: 0.0276\n",
      "Epoch [88/100], Step [600/22500], Loss: 0.0252\n",
      "Epoch [88/100], Step [700/22500], Loss: 0.0147\n",
      "Epoch [89/100], Step [100/22500], Loss: 0.0200\n",
      "Epoch [89/100], Step [200/22500], Loss: 0.0201\n",
      "Epoch [89/100], Step [300/22500], Loss: 0.0179\n",
      "Epoch [89/100], Step [400/22500], Loss: 0.0279\n",
      "Epoch [89/100], Step [500/22500], Loss: 0.0215\n",
      "Epoch [89/100], Step [600/22500], Loss: 0.0282\n",
      "Epoch [89/100], Step [700/22500], Loss: 0.0266\n",
      "Epoch [90/100], Step [100/22500], Loss: 0.0147\n",
      "Epoch [90/100], Step [200/22500], Loss: 0.0170\n",
      "Epoch [90/100], Step [300/22500], Loss: 0.0145\n",
      "Epoch [90/100], Step [400/22500], Loss: 0.0364\n",
      "Epoch [90/100], Step [500/22500], Loss: 0.0283\n",
      "Epoch [90/100], Step [600/22500], Loss: 0.0214\n",
      "Epoch [90/100], Step [700/22500], Loss: 0.0310\n",
      "Epoch [91/100], Step [100/22500], Loss: 0.0344\n",
      "Epoch [91/100], Step [200/22500], Loss: 0.0272\n",
      "Epoch [91/100], Step [300/22500], Loss: 0.0220\n",
      "Epoch [91/100], Step [400/22500], Loss: 0.0176\n",
      "Epoch [91/100], Step [500/22500], Loss: 0.0404\n",
      "Epoch [91/100], Step [600/22500], Loss: 0.0238\n",
      "Epoch [91/100], Step [700/22500], Loss: 0.0256\n",
      "Epoch [92/100], Step [100/22500], Loss: 0.0183\n",
      "Epoch [92/100], Step [200/22500], Loss: 0.0215\n",
      "Epoch [92/100], Step [300/22500], Loss: 0.0222\n",
      "Epoch [92/100], Step [400/22500], Loss: 0.0239\n",
      "Epoch [92/100], Step [500/22500], Loss: 0.0174\n",
      "Epoch [92/100], Step [600/22500], Loss: 0.0185\n",
      "Epoch [92/100], Step [700/22500], Loss: 0.0306\n",
      "Epoch [93/100], Step [100/22500], Loss: 0.0203\n",
      "Epoch [93/100], Step [200/22500], Loss: 0.0212\n",
      "Epoch [93/100], Step [300/22500], Loss: 0.0234\n",
      "Epoch [93/100], Step [400/22500], Loss: 0.0263\n",
      "Epoch [93/100], Step [500/22500], Loss: 0.0228\n",
      "Epoch [93/100], Step [600/22500], Loss: 0.0239\n",
      "Epoch [93/100], Step [700/22500], Loss: 0.0167\n",
      "Epoch [94/100], Step [100/22500], Loss: 0.0292\n",
      "Epoch [94/100], Step [200/22500], Loss: 0.0193\n",
      "Epoch [94/100], Step [300/22500], Loss: 0.0224\n",
      "Epoch [94/100], Step [400/22500], Loss: 0.0268\n",
      "Epoch [94/100], Step [500/22500], Loss: 0.0116\n",
      "Epoch [94/100], Step [600/22500], Loss: 0.0230\n",
      "Epoch [94/100], Step [700/22500], Loss: 0.0252\n",
      "Epoch [95/100], Step [100/22500], Loss: 0.0455\n",
      "Epoch [95/100], Step [200/22500], Loss: 0.0205\n",
      "Epoch [95/100], Step [300/22500], Loss: 0.0177\n",
      "Epoch [95/100], Step [400/22500], Loss: 0.0125\n",
      "Epoch [95/100], Step [500/22500], Loss: 0.0494\n",
      "Epoch [95/100], Step [600/22500], Loss: 0.0217\n",
      "Epoch [95/100], Step [700/22500], Loss: 0.0249\n",
      "Epoch [96/100], Step [100/22500], Loss: 0.0118\n",
      "Epoch [96/100], Step [200/22500], Loss: 0.0244\n",
      "Epoch [96/100], Step [300/22500], Loss: 0.0280\n",
      "Epoch [96/100], Step [400/22500], Loss: 0.0146\n",
      "Epoch [96/100], Step [500/22500], Loss: 0.0102\n",
      "Epoch [96/100], Step [600/22500], Loss: 0.0212\n",
      "Epoch [96/100], Step [700/22500], Loss: 0.0187\n",
      "Epoch [97/100], Step [100/22500], Loss: 0.0212\n",
      "Epoch [97/100], Step [200/22500], Loss: 0.0425\n",
      "Epoch [97/100], Step [300/22500], Loss: 0.0157\n",
      "Epoch [97/100], Step [400/22500], Loss: 0.0408\n",
      "Epoch [97/100], Step [500/22500], Loss: 0.0180\n",
      "Epoch [97/100], Step [600/22500], Loss: 0.0142\n",
      "Epoch [97/100], Step [700/22500], Loss: 0.0280\n",
      "Epoch [98/100], Step [100/22500], Loss: 0.0114\n",
      "Epoch [98/100], Step [200/22500], Loss: 0.0076\n",
      "Epoch [98/100], Step [300/22500], Loss: 0.0179\n",
      "Epoch [98/100], Step [400/22500], Loss: 0.0216\n",
      "Epoch [98/100], Step [500/22500], Loss: 0.0133\n",
      "Epoch [98/100], Step [600/22500], Loss: 0.0179\n",
      "Epoch [98/100], Step [700/22500], Loss: 0.0237\n",
      "Epoch [99/100], Step [100/22500], Loss: 0.0180\n",
      "Epoch [99/100], Step [200/22500], Loss: 0.0174\n",
      "Epoch [99/100], Step [300/22500], Loss: 0.0220\n",
      "Epoch [99/100], Step [400/22500], Loss: 0.0082\n",
      "Epoch [99/100], Step [500/22500], Loss: 0.0203\n",
      "Epoch [99/100], Step [600/22500], Loss: 0.0290\n",
      "Epoch [99/100], Step [700/22500], Loss: 0.0129\n",
      "Epoch [100/100], Step [100/22500], Loss: 0.0506\n",
      "Epoch [100/100], Step [200/22500], Loss: 0.0191\n",
      "Epoch [100/100], Step [300/22500], Loss: 0.0323\n",
      "Epoch [100/100], Step [400/22500], Loss: 0.0084\n",
      "Epoch [100/100], Step [500/22500], Loss: 0.0212\n",
      "Epoch [100/100], Step [600/22500], Loss: 0.0148\n",
      "Epoch [100/100], Step [700/22500], Loss: 0.0338\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, epochs, i+1, len(train_dataset)//32, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 95.24833333333333 %\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        outputs = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (outputs == labels).sum().item()\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = torch.Tensor(df_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model on X_test\n",
    "with torch.no_grad():\n",
    "    #X_test = torch.Tensor(X_test).to(device)\n",
    "    y_pred = model(df_submission.to(device))\n",
    "    y_pred = (y_pred > 0.5).float()\n",
    "    #y_pred = y_pred.numpy()\n",
    "    #y_pred = y_pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700000,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['target'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_df[['id', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8083148c435d231f703449331507e72b23a2d1daf1c5ab6243dbd5548bf4abd5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
